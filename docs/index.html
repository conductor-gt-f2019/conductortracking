<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
  <link href="css/main.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1 align='left'>Predicting time signature from musical conducting using pattern classification with accumulator matrix</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!--Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, beause you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained-->

<!-- Goal -->
<h3>Abstract</h3>

<!--One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained.-->

In music, conducting is the art of directing the simultaneous performance of several players or singers by the use of gesture [1]. A conductor provides visual cues which help the orchestra to play in time and provide information about the variations in dynamics of sound. These visual cues are analogous to hand gestures. In order for a machine or a robot to interact with a human player or with an orchestra, they need a way to understand a conductor's cues in real-time. One common cue that needs to be understood is the time signature of the piece. The time signature indicates the number of beats in each measure in a piece of music. Conductors use different gestures to denote different time signatures. We took a subset of commonly used time signatures (2/4, 3/4, 4/4) and classify them within a video stream of a conductor performing in real-time. With our method, we obtained an average accuracy of about 95.641% with the dataset that includes recording ourselves conducting and also conducting videos from youtube.

<br><br><br>
<!-- figure -->
<!--
<h3>Teaser figure</h3>
A figure that conveys the main idea behind the project or the main application being addressed.
<br><br>
-->
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 240px;" alt="" src="teaser.png">
</div>

<!--<br><br>-->
<!-- Introduction -->
<!--
<h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new.
-->

<h3>Introduction</h3>

In our project, we present a novel method for automatically detecting three of the most common time signatures in musical conducting (2/4, 3/4, 4/4). Our method utilizes a series of Computer Vision capabilities, namely the OpenPose pose-estimation framework and a downstream image classifier which receives pose coordinates from the conductor's gestures as input. Our method detects the correct time signature gesture with high accuracy. In the following summary, we describe in detail our proposed method and present the results of our research experiments. Lastly, we suggest further research opportunities utilizing our method.


<br><br><br>
<!-- Approach -->
<h3>Approach</h3>
<!-- Describe very clearly and systematically your approach to solve the problem. Tell us exactly what existing implementations you used to build your system. Tell us what obstacles you faced and how you addressed them. Justify any design choices or judgment calls you made in your approach. -->

We used the data driven approach to solve the problem. This is because modelling the movement patterns mathematically is very difficult and won't be effective due to large variations in the patterns - noise. We trained a ResNet18[2] model with accumulator matrices which embed the movement patterns. Note that we will refer to a complete time-signature pattern as gesture in the future. The accumulator matrix is formed by using votes for each point in the matrix that correspond to the spacial movement of the wrist as captured by the images over time (this is discussed in detail in preprocessing). The wrist points are detected using CMU's openpose model [3]. We would also perform polynomial interpolation to fix discontinuity in detection of wrist points. The network will be trained to classify these patterns and predict the time signature.


<br><br><br>
<!-- Results -->
<h3>Experiments and Results</h3>
<h5>Data Collection</h4>
For each time signature, we recorded approximately 15 minutes of footage (480p, 30fps) of ourselves performing the corresponding gesture. We then used OpenPose[5] to extract the x and y coordinates of our wrists in each video.
<br>

<h5>Data Cleaning and blocking</h5>
The data that we recieve from OpenPose is noisy. Sometimes OpenPose is not able to detect the wrists and so gives output as 0 for x and y coordinates. If the time for which the wrists are not detected is larger then we remove these coordinates. The data was then blocked into chunks to include 1 full cycle of the pattern. This ensures each accumulator matrix would have one full cycle of the gesture pattern.<br>

<h5>Creating Accumulator Matrices</h5>
Once we blocked the data, we created an accumulator matrix like representation for the motion that was traced by the point in each block. We then normalized the spread of the pose points in the accumulator matrix to a 500x500 matrix to reduce the effects of scale on classification. The normalized accumulator matrix was saved as a 500x500 pixel image to prepare it for use in our deep learning model. We computed one matrix for each block. The video below shows the generated accumulated matrices for all the csv files.

<h5>Additional Processing</h5>
The conducting gestures can be performed with either hand, so we also saved a horizontally flipped copy of each accumulator matrix image.

<div style="display: flex; justify-content: center;">
  <video controls width="480">
    <source src="AccMatViz.mp4" type="video/mp4">
  </video>
</div>

<br>
<br>
<h5>Training</h5>

As we did not have enough data to train a model from scratch, we used the transfer learning technique to train a pretrained Resnet18. It was pretrained on the ImageNet dataset. We initialized and trained only the fully connected layer, retaining the convolutional layers from the pretrained model. Since this is a classification problem, we used the cross-entropy loss function. We used Adam for the optimization with a learning rate of 0.001. We also used a LRScheduler to step down the learning rate every 7 epochs by 0.1. We augemented the data by shifting and scaling the accumulator matrices randomly to reduce overfitting and to improve generality.
<br><br>

<h5>Evaluation</h5>
To evaluate the model performance, we used the Support Vector Machines as our baseline. We used scikit-learn's[4] implementation of SVM. With the default parameters, we obtained an overall accuracy of 83.6%. The prediction accuracy per class were 69.23%, 87.69%, 86.15% for 2/4, 3/4 and 4/4 respectively. The resnet model performed better with an overall accuracy of about 95.641%. The confusion matrix for the classification is show in Fig 1. The loss curve is shown in Fig 2. This curve trend suggests the model is not overfitting.
<br><br>
<div style="text-align: center;">
<h6>Fig 2 Training and validation loss</h6>
<img style="height: 480px;" alt="" src="loss_curve_plot.png">
</div>

<h6>Fig 1 Confusion Matrix for the resnet18 model</h6>
<div style="display: flex; flex-direction: row;">
    <div>
        <table border="1">
            <thead>
            <tr>
            <th></th>
            <th>2/4</th>
            <th>3/4</th>
            <th>4/4</th>
            <th>Accuracy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <th>2/4</th>
            <td>118</td>
            <td>10</td>
            <td>20</td>
            <td>0.907692</td>
            </tr>
            <tr>
            <th>3/4</th>
            <td>4</td>
            <td>125</td>
            <td>1</td>
            <td>0.961538</td>
            </tr>
            <tr>
            <th>4/4</th>
            <td>0</td>
            <td>0</td>
            <td>130</td>
            <td>1.000000</td>
            </tr>
            </tbody>
            </table>
    </div>
</div>
<br><br>

<h3>Qualitative results</h3>
Here are examples of our model being used to predict gestures based on a time series of point data. It can be seen that the model classifies the data correctly in most cases. Some failure and fluctuations can be notices in the 2/4 demo video especially. This could be due to the pattern being in close proximity with that of the patterns in 3/4 and 4/4.
<div style="display: flex; flex-direction: row; justify-content: center;">
  <div style="display: flex; flex-direction: column; justify-content: center; text-align: center;">
    <h6>2-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_2-2.mp4" type="video/mp4">
    </video>
  </div>
  <div style="display: flex; flex-direction: column; justify-content: center;">
    <h6>3-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_3-4.mp4" type="video/mp4">
    </video>
  </div>
  <div style="display: flex; flex-direction: column; justify-content: center;">
    <h6>4-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_4-4.mp4" type="video/mp4">
    </video>
  </div>
</div>
<br><br>

<h3>Conclusion and Future work</h3>
In this project, we proposed an efficient technique to classify the most common time signatures (2/4, 3/4, 4/4) used in music. We trained a pre-trained resnet18 model to classify gestures using the accumulator matrix that were created from the x, y coordinates of the hand positions.
<br><br>
Our next step would be to include more time signatures such as the 5/4, 6/8, etc. We would also like to experiment more on the minimum required block size to successfully classify the gesture instead of using one cycle of pattern. The jittery output that we notice in ther video is because there is no temporal coherence between the predictions. We would include recurrent architectures that enable learning temporal data to reduce the jitteriness in prediction.
<br><br>
The proposed method can be applied to robotics where the robot musician have to synchronize to the human player. It can also be used as an educative tool to teach the art of conducting!
<br><br>

<h3>References</h3>
<ol>
  <li>The Art of Conducting (Garden City, New York: Doubleday, 1959); English edition as The Conductor: His Artistry and Craftsmanship (London: G. Bell & Sons, 1961).</li>
  
  <li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li>

 <li>Cao, Zhe, et al. "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields." arXiv preprint arXiv:1812.08008 (2018).</li>
 
 <li>API design for machine learning software: experiences from the scikit-learn project, Buitinck et al., 2013.</li>
 
 <li>https://github.com/CMU-Perceptual-Computing-Lab/openpose</li>

</ol>


  <hr>
  <footer> 
  <p>© Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
