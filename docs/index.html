<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
  <link href="css/main.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1 align='left'>Predicting time signature from musical conducting using pattern classification with accumulator matrix</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!--Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, beause you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained-->

<!-- Goal -->
<h3>Abstract</h3>

<!--One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained.-->

In music, conducting is the art of directing the simultaneous performance of several players or singers by the use of gesture [1]. A conductor provides visual cues which help the orchestra to play in time and provide information about the variations in dynamics of sound. These visual cues are analogous to hand gestures. In order for a machine or a robot to interact with a human player or with an orchestra, they need a way to understand a conductor's cues in real-time. One common cue that needs to be understood is the time signature of the piece. The time signature indicates the number of beats in each measure in a piece of music. Conductors use different gestures to denote different time signatures. We took a subset of commonly used time signatures (2/4, 3/4, 4/4) and classify them within a video stream of a conductor performing in real-time.  

<br><br><br>
<!-- figure -->
<!--
<h3>Teaser figure</h3>
A figure that conveys the main idea behind the project or the main application being addressed.
<br><br>
-->
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 240px;" alt="" src="teaser.png">
</div>

<!--<br><br>-->
<!-- Introduction -->
<!--
<h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new.
-->

<h3>Introduction</h3>

In our project, we present a novel method for automatically detecting three of the most common time signatures in musical conducting (2/4, 3/4, 4/4). Our method utilizes a series of Computer Vision capabilities, namely the OpenPose pose-estimation framework and a downstream image classifier which receives pose coordinates from the conductor's gestures as input. Our method detects the correct time signature gesture with high accuracy. In the following summary, we describe in detail our proposed method and present the results of our research experiments. Lastly, we suggest further research opportunities utilizing our method.


<br><br><br>
<!-- Approach -->
<h3>Approach</h3>
<!-- Describe very clearly and systematically your approach to solve the problem. Tell us exactly what existing implementations you used to build your system. Tell us what obstacles you faced and how you addressed them. Justify any design choices or judgment calls you made in your approach. -->

We used the data driven approach to solve the problem. This is because modelling the movement patterns mathematically is very difficult and won't be effective due to large variations in the patterns - noise. We trained a ResNet18[2] model with accumulator matrices which embed the movement patterns. Note that we will refer to a complete time-signature pattern as gesture in the future. The accumulator matrix is formed by using votes for each point in the matrix that correspond to the spacial movement of the wrist as captured by the images over time (this is discussed in detail in preprocessing). The wrist points are detected using CMU's openpose model [3]. We would also perform polynomial interpolation to fix discontinuity in detection of wrist points. The network will be trained to classify these patterns and predict the time signature.


<br><br><br>
<!-- Results -->
<h3>Experiments and results</h3>
<h4>Data Collection</h4>
<p>For each time signature, we recorded approximately 15 minutes of footage (480p, 30fps) of ourselves performing the corresponding gesture. We then used <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a> to extract the x and y coordinates of our wrists in each video.</p>
<br>

<h5>Data Cleaning:</h5>
The data that we recieve from OpenPose is noisy. Sometimes OpenPose is not able to detect the wrists and so gives output as 0 for x and y coordinates. If the time for which the wrists are not detected is larger then we remove these coordinates.<br>

<h5>Blocking:</h5>
The data stream is then windowed and converted into smaller chunks or blocks. The block size and hop size for this process can be controlled. The block size has to be such that it encompasses atleast one complete iteration of the gesture being performed. This is a hyperparameter and we will tune it during training and validation. Time signature is independent of the tempo and since music can be at various tempos, the amount of action captured in a block will vary with it. Music at lower tempo will need more time to complete one full cycle of the gesture, so if we take the block size large enough to account for the lowest tempo we should be fine to start with. For visualization in the videos below, we have used a blocksize of 100.<br>

<h5>Creating Accumulator Matrices:</h5>
Once we blocked the data, we created an accumulator matrix like representation for the motion that was traced by the point in each block. We then normalized the spread of the pose points in the accumulator matrix to a 500x500 matrix to reduce the effects of scale on classification. The normalized accumulator matrix was saved as a 500x500 pixel image to prepare it for use in our deep learning model. We computed one matrix for each block. The video below shows the generated accumulated matrices for all the csv files.

<h5>Additional Processing:</h5>
The conducting gestures can be performed with either hand, so we also saved a horizontally flipped copy of each accumulator matrix image.

<div style="display: flex; justify-content: center;">
  <video controls width="480">
    <source src="AccMatViz.mp4" type="video/mp4">
  </video>
</div>

<br>
<br>
<h4>Training & Experiments:</h4>

As we did not have enough data to train a model from scratch, we used the transfer learning technique to train a pretrained Resnet18. It was pretrained on the ImageNet dataset. We initialized and trained only the fully connected layer, retaining the convolutional layers from the pretrained model. 

<br><br>
Since this is a classification problem, we used the cross-entropy loss function. We used Adam for the optimization with a learning rate of 0.001. We also used a LRScheduler to step down the learning rate every 7 epochs by 0.1
<br><br>
We will experiment with different block sizes for segmenting the input data. We have assumed that we need atleast one complete iteration of the gesture in a block. We will test this assumption by using smaller blocks that don't cover a full cycle of the time signature gesture.
<br><br>

<h3>Evaluation</h3>
To evaluate the model performance, we used the Support Vector Machines as our baseline. We used scikit-learn's[4] implementation of SVM. With the default parameters, we obtained an overall accuracy of 83.6%. The prediction accuracy per class were 69.23%, 87.69%, 86.15% for 2/4, 3/4 and 4/4 respectively.

<!-- <h6>Algorithmic Approach to Gesture Detection</h6>
For classifications problems being solved with CNN models, it is helpful to have an algorithmic approach as a frame of reference for success and accuracy. One idea that we had for classifying a conductor's gestures was to detect the "inflection points" of a conductor's hand motions. When a conductor moves their hands to coordinate the next beat, they quickly move their hands in a different angle and stop at a different location in space. The order of these stopping locations can be used to detect the time signature.
<br><br>
We started by creating a script that watches a person's hand motions for several frames. To conduct a certain time signature, a person must move their hands in a "loop" for each bar of music. This loop can be used to form a bounding box.
<br><br>
<div style="display: flex; justify-content: center;"><img style="height: 240px;" alt="" src="bounding_box.png"></div>
<br>
Reexamining 2/4 and 3/4 time signatures with a bounding box, we can see that both gestures basically involve moving one's hand to different corners of the bounding box. The motion of the hand will slow down as it approaches the corner, and so will the angle of the movement. This is an inflection point. By tracking these inflection points over time, we can pattern match our set of inflection points with the specific order of inflection points expected by each time signature. For a 2/4 time signature, the hand will move from the top-left, to the bottom-right, and then back to the top-left. For a 3/4 time signature, the hand will move from the top-left, bottom-left, bottom-right, and then back to the top-left. With all this in mind, we created a preliminary demonstration, shown below.
<br><br>
<div style="display: flex; justify-content: center;">
  <video controls width="640">
    <source src="detect_twofourthreefour.mp4" type="video/mp4">
  </video>
</div>
<br><br>
There are some weaknesses to this approach. Some threshold parameters have to be manually tuned in order to define what counts as "moving slowly near a corner". The algorithm also does not produce accurate detection until it has time to define a bounding box. We can see this being a problem if the conductor changes their stance to face a different part of the orchestra. In other words, it can be difficult to track the gesture if the camera isn't facing the conductor head-on. Also, this method can be limited by the framerate of the camera. Fast-paced music requires a conductor to move their hands quickly, and if the camera can't keep up with that, the algorithm does not detect the change in speed and angle accurately. We are still working on detection for a 4/4 time signature, and we are working on testing the algorithm on a larger set of data to gain empirical results on its accuracy.
<br><br>
The source code for the algorithm can be found at <a href="https://github.com/conductor-gt-f2019/OpenPoseTest">https://github.com/conductor-gt-f2019/OpenPoseTest</a>
<div style="display: flex; justify-content: center;">
  <video controls width="480">
    <source src="34_sample.mp4" type="video/mp4">
  </video>
</div> -->


<!-- Main Results Figure --> 
<!--
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="results.png">
</div>
<br><br>
-->

<h6>Confusion Matrix for the resnet18 model</h6>
<div style="display: flex; flex-direction: row;">
    <div>
        <table border="1">
            <thead>
            <tr>
            <th></th>
            <th>2/4</th>
            <th>3/4</th>
            <th>4/4</th>
            <th>Accuracy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <th>2/4</th>
            <td>118</td>
            <td>10</td>
            <td>20</td>
            <td>0.907692</td>
            </tr>
            <tr>
            <th>3/4</th>
            <td>4</td>
            <td>125</td>
            <td>1</td>
            <td>0.961538</td>
            </tr>
            <tr>
            <th>4/4</th>
            <td>0</td>
            <td>0</td>
            <td>130</td>
            <td>1.000000</td>
            </tr>
            </tbody>
            </table>
    </div>
</div>
<br><br>

<h3>Qualitative results</h3>
Here are examples of our model being used to predict gestures based on a time series of point data.
<div style="display: flex; flex-direction: row; justify-content: center;">
  <div style="display: flex; flex-direction: column; justify-content: center; text-align: center;">
    <h6>2-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_2-2.mp4" type="video/mp4">
    </video>
  </div>
  <div style="display: flex; flex-direction: column; justify-content: center;">
    <h6>3-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_3-4.mp4" type="video/mp4">
    </video>
  </div>
  <div style="display: flex; flex-direction: column; justify-content: center;">
    <h6>4-4 Time Signature</h6>
    <video controls width="400">
      <source src="demo_4-4.mp4" type="video/mp4">
    </video>
  </div>
</div>
<br><br>

<br>
<h3>References</h3>
<ol>
  <li>The Art of Conducting (Garden City, New York: Doubleday, 1959); English edition as The Conductor: His Artistry and Craftsmanship (London: G. Bell & Sons, 1961).</li>
  
  <li>He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li>

 <li>Cao, Zhe, et al. "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields." arXiv preprint arXiv:1812.08008 (2018).</li>
 
 <li>API design for machine learning software: experiences from the scikit-learn project, Buitinck et al., 2013.</li>

</ol>


  <hr>
  <footer> 
  <p>Â© Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
