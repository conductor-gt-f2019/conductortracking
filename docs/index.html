<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1 align='left'>Predicting time signature from musical conducting using pattern classification with accumulator matrix</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!--Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, beause you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained-->

<!-- Goal -->
<h3>Abstract</h3>

<!--One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained.-->

In music, conducting is the art of directing the simultaneous performance of several players or singers by the use of gesture [1]. A conductor provides visual cues which help the orchestra to play in time and provide information about the variations in dynamics of the sound. These visual ques are nothing but various hand gestures. In order for a machine or a robot to interact with a human player or with an orchestra, they need a way to understand a conductor's cues in real-time. One common cue that needs to be understood is the time signature of the piece. The time signature indicates the number of beats in each measure in a piece of music. Conductors use different gestures to denoe different time signatures. We propose to take a subset of commonly used time signatures — 2/4, 3/4, 4/4, 5/4, 7/8 — and classify them in a video stream of a conductor performing in real-time.  

<br><br>
<!-- figure -->
<!--
<h3>Teaser figure</h3>
A figure that conveys the main idea behind the project or the main application being addressed.
<br><br>
-->
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 240px;" alt="" src="teaser.png">
</div>

<!--<br><br>-->
<!-- Introduction -->
<!--
<h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new.
-->

<br><br>
<!-- Approach -->
<h3>Approach</h3>
<!-- Describe very clearly and systematically your approach to solve the problem. Tell us exactly what existing implementations you used to build your system. Tell us what obstacles you faced and how you addressed them. Justify any design choices or judgment calls you made in your approach. -->
We intend to use a data driven approach to solve the problem. This is because modelling the movement patterns mathematically is very difficult and won't be effective due to large variations in the patterns - noise. We plan to train a convolutional neural network model with accumulator matrices which embed the movement patterns. The accumulator matrix is formed by using votes for each point in the matrix that correspond to the spacial movement of the wrist as captured by the images over time. The wrist points are detected using CMU's openpose algorithm. We would also perform polynomial interpolation to fix discontinuity in detection of wrist points. The network is trained to classify these patterns and predict the time signature. Added to that, we plan to experiment with 2 approaches for the No-time-signature class. This is when the conductor either is not performing or is performing a piece that is not time synced (ex: rubato, improvisation). One approach is to have a seperate class and training the model with random patterns that are not part of other classes used. The second approach is to just have the time signature classes and to set a threshold of confidence to detect "no-class" 


<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
<!-- Provide details about the experimental set up (number of images/videos, number of datasets you experimented with, train/test split if you used machine learning algorithms, etc.). Describe the evaluation metrics you used to evaluate how well your approach is working. Include clear figures and tables, as well as illustrative qualitative examples if appropriate. Be sure to include obvious baselines to see if your approach is doing better than a naive approach (e.g. for classification accuracy, how well would a classifier do that made random decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance varies as you change those parameter values. Be sure to discuss any trends you see in your results, and explain why these trends make sense. Are the results as expected? Why? -->

<br>
<h5>Data Collection and Preprocessing:</h5>

For training, we need video recordings of conductors performing the subset of gestures that we plan to use. For each gesture in the subset we need multiple instances from multiple conductors. We plan to collect this data by recording ourselves performing each gesture over and over. For each gesture we would have a long video of repeated actions. We will use OpenPose [2] to extract the skeletal image information for the video. This will help us to detect the hand motion in the form of a point. For the whole video, we'd get the time series data of coordinates of the point that tracks the hand.<br>
We'll divide this series of data into blocks of a certain size. This size will have to be such that it encompasses atleast one complete iteration of the gesture being performed. The challenge here is to decide on a block length. Time signature is independent of tempo and since music can be at various tempos, the amount of action captured in a block will vary with it. Music at lower tempo will  need more time for the gesture to complete, so if we take teh block size large enough to account for the lowest tempo we should be fine.<br>
Once we have blocked the data, we will create an accumulator matrix like representation for the motion that is traced by the point in each block. This accumulator matrix image will be fed to the CNN as input. This is the pattern the we want our network to learn and detect.


<h5>Training and Experiments:</h5>

The network would be trained to classify a given gesture from the accumulator matrix. In real world scenario we might get multiple gestures in the same video and our model may not know many of these. In this case we need our model to predict 'none'. This can be handled by either setting a threshold for the class prediction or by adding another 'None' class to out input. Experiments need to be done to see which of these methods works best.




<!-- Main Results Figure --> 
<!--
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="results.png">
</div>
<br><br>
-->

<!-- Results -->
<!--
<h3>Qualitative results</h3>
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.
<br><br>
-->

<!-- Main Results Figure --> 
<!--
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br>
-->

<h3>References</h3>
<ol>
  <li>The Art of Conducting (Garden City, New York: Doubleday, 1959); English edition as The Conductor: His Artistry and Craftsmanship (London: G. Bell & Sons, 1961).</li>

 <li>Cao, Zhe, et al. "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields." arXiv preprint arXiv:1812.08008 (2018).</li>

</ol>


  <hr>
  <footer> 
  <p>© Raghavasimhan Sankaranarayanan, Kaushal Sali, Christopher Dixon, Phong Tran</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
